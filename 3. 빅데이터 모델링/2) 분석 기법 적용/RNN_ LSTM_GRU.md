# RNN, LSTM, GRU 비교 
> 자주 등장하는 **RNN, LSTM, GRU**의 차이점을 깔끔하게 비교
> 이 세 가지는 모두 **순차 데이터(시계열, 텍스트 등)**를 처리하는 데 사용되며, 각각의 구조와 특징이 조금씩 다르다.

---

## 🧠 핵심 비교표

| 항목            | RNN (Recurrent Neural Network) | LSTM (Long Short-Term Memory) | GRU (Gated Recurrent Unit) |
|-----------------|-------------------------------|-------------------------------|-----------------------------|
| 🔁 구조         | 단순 순환 구조                 | 셀 상태 + 3개의 게이트        | 2개의 게이트만 사용         |
| 🧱 게이트 구성   | 없음                           | 입력, 삭제, 출력 게이트       | 업데이트, 리셋 게이트       |
| 🧠 기억 능력     | 짧은 기억만 가능               | 장기 기억 가능                | 장기 기억 가능 (LSTM보다 간단) |
| ⚠️ 문제점 해결   | 기울기 소실 문제 있음          | 기울기 소실 해결 가능         | 기울기 소실 해결 가능       |
| ⚡ 연산 속도     | 빠름                           | 느림 (구조 복잡)             | 빠름 (구조 간단)           |
| 📊 파라미터 수  | 적음                           | 많음                          | 중간                        |
| 🧪 성능         | 낮음                           | 높음                          | LSTM과 유사하거나 더 좋음   |

---

## 🔍 간단 설명

- **RNN** :  
  - 가장 기본적인 순환 신경망.  
  - 이전 정보를 기억하지만 시간이 지나면 잊어버림.  
  - **기울기 소실 문제** 로 긴 문장이나 시계열에 약함.

- **LSTM** :  
  - RNN의 한계를 극복하기 위해 고안됨.  
  - **셀 상태(cell state)** 를 통해 정보를 오래 기억함.  
  - **3개의 게이트**로 정보 흐름을 정교하게 제어함.

- **GRU** :  
  - LSTM을 간소화한 버전.  
  - **2개의 게이트** 만 사용해 계산이 더 빠름.  
  - 성능은 LSTM과 비슷하거나 더 좋을 때도 있음.

---

## 🧠 선택 팁

- **데이터가 길고 복잡하다면** → LSTM  
- **빠른 학습과 적은 자원이 필요하다면** → GRU  
- **간단한 문제나 실험용이라면** → RNN

---
