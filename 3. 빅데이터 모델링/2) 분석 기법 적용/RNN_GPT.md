# RNN 부터 GPT
> 딥러닝 모델의 진화 과정을 RNN부터 GPT까지
> 복잡한 수식은 잠시 접어두고, 핵심 개념과 차이점을 중심으로 정리

---

## 🔄 RNN (Recurrent Neural Network) – 순차적 데이터 전문가

- **핵심 개념**: 과거 정보를 기억하며 현재 입력을 처리하는 구조
- **특징**:
  - 시간 순서가 있는 데이터(예: 문장, 음성)에 강함
  - 은닉 상태(hidden state)를 통해 이전 정보를 전달
- **단점**:
  - 긴 문장에서는 기억이 희미해지는 **장기 의존성 문제**
  - 병렬 처리 어려움 → 느린 학습
- **보완 모델**:
  - **LSTM**: 기억을 오래 유지하도록 만든 구조
  - **GRU**: LSTM보다 간단하면서도 비슷한 성능

---

## 🧲 Attention & Transformer – 문맥을 더 똑똑하게 이해하다

- **Attention**: 문장의 모든 단어를 서로 비교해 중요한 부분에 집중
- **Transformer 구조**:
  - RNN 없이도 문맥을 이해할 수 있음
  - 병렬 처리 가능 → 빠르고 효율적
  - Encoder-Decoder 구조로 번역, 요약 등에 활용

---

## 💬 GPT (Generative Pre-trained Transformer) – 생성형 AI의 대표주자

- **핵심 원리**:
  - Transformer 기반
  - Self-Attention으로 문맥 파악
  - 사전 학습 + 미세 조정으로 다양한 작업 수행
- **특징**:
  - 문장 생성, 요약, 번역, 코딩까지 가능
  - GPT-3는 1750억 개의 파라미터, GPT-4는 그 이상!
- **활용 분야**:
  - ChatGPT 같은 대화형 AI
  - 자동 이메일 작성, 블로그 생성, 코드 리뷰 등

---

## 📊 한눈에 비교

| 모델 | 구조 | 강점 | 주요 활용 |
|------|------|------|-----------|
| RNN | 순환 구조 | 시계열 데이터 처리 | 번역, 감성 분석, 예측 |
| LSTM/GRU | 개선된 RNN | 장기 기억 유지 | 긴 문장 처리 |
| Transformer | Attention 기반 | 병렬 처리, 문맥 이해 | 번역, 요약 |
| GPT | Transformer 기반 | 자연어 생성, 창의적 작업 | 챗봇, 문서 작성, 코딩 |

---
