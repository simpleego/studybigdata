# 필터기법
> 데이터의 통계적 측정 방법을 사용하여 특징들의 상관관계를 알아낸 뒤 높은 상관관계를 갖는 특징을 선택하는 방법
- 정보이득
- 카이제곱 검정
- 피셔 스코어
- 상관계수

### 정보 이득(Information Gain)

정보 이득은 **의사결정 나무(Decision Tree)**와 같은 모델에서 **특징 선택(Feature Selection)**에 사용되는 중요한 개념입니다.   
쉽게 말해, 어떤 특징(변수)을 기준으로 데이터를 나눌 때, 그 데이터가 얼마나 **깨끗하게** 혹은 **순수하게** 분류되는지를 측정하는 지표입니다.

카이제곱 검정이나 피셔 스코어가 각각 범주형, 연속형 변수의 유용성을 평가하는 것처럼,  
정보 이득은 특히 **분류(Classification) 문제**에서 특정 특징이 클래스를 얼마나 잘 구별해 내는지 측정합니다.

---

### 정보 이득의 핵심 아이디어

정보 이득은 **엔트로피(Entropy)**라는 개념을 기반으로 계산됩니다.

* **엔트로피:** 데이터가 얼마나 섞여 있는지, 즉 **불확실성**이나 **혼잡도**를 나타내는 척도입니다.
    * 엔트로피가 **높다**는 것은 데이터가 여러 클래스로 **뒤죽박죽 섞여 있다**는 의미입니다.
    * 엔트로피가 **낮다**는 것은 데이터가 한 클래스로 **매우 순수하게** 모여 있다는 의미입니다.

정보 이득은 특정 특징으로 데이터를 나눈 후, **'나누기 전의 엔트로피'**와 **'나눈 후의 엔트로피'**를 비교하여 계산합니다.

$$\text{정보 이득} = \text{나누기 전의 엔트로피} - \text{나눈 후의 엔트로피의 평균}$$

정보 이득은 이 값이 **클수록** 좋습니다. 왜냐하면 불확실성이 높은 상태(엔트로피 높음)에서 불확실성이 낮은 상태(엔트로피 낮음)로 바뀌었다는 뜻이기 때문입니다. 즉, 데이터를 나눈 결과가 더 '순수'해졌다는 것을 의미합니다.

---

### 간단한 예시: 날씨로 놀러갈지 결정하기

우리가 날씨 데이터(맑음, 비)와 놀러 갈지 말지(간다, 안 간다)에 대한 과거 기록을 가지고 있다고 가정해 봅시다.

| 날씨 | 놀러감 |
| :--- | :----- |
| 맑음 | 간다   |
| 맑음 | 간다   |
| 맑음 | 간다   |
| 비   | 안 간다 |
| 비   | 안 간다 |

1.  **나누기 전의 엔트로피 계산**
    * 총 5개의 데이터가 있습니다.
    * '간다'가 3개, '안 간다'가 2개로 섞여 있으므로 엔트로피가 어느 정도 있습니다. (불확실성이 높음)

2.  **'날씨' 특징으로 나눈 후의 엔트로피 계산**
    * **'맑음' 그룹:** 3개의 데이터가 모두 '간다'입니다. 이 그룹의 엔트로피는 **0**입니다. (매우 순수함)
    * **'비' 그룹:** 2개의 데이터가 모두 '안 간다'입니다. 이 그룹의 엔트로피는 **0**입니다. (매우 순수함)
    * **나눈 후의 엔트로피 평균:** 각 그룹의 엔트로피를 합산하여 평균을 냅니다. 이 예시에서는 **0**이 됩니다.

3.  **정보 이득 계산**
    * 정보 이득 = (나누기 전의 엔트로피) - (나눈 후의 엔트로피의 평균)
    * 정보 이득 = (어느 정도 불확실한 값) - 0
    * 결과적으로 정보 이득은 **높은 값**이 됩니다.

이처럼 '날씨'라는 특징으로 데이터를 나누었더니, 각 그룹의 엔트로피가 0이 될 정도로 '놀러감/안 놀러감'이 완벽하게 분류되었습니다.  
따라서 정보 이득은 높게 나타나고, **'날씨'는 놀러 갈지 말지를 결정하는 가장 중요한 특징**으로 선택될 수 있습니다.

의사결정 나무는 이 과정을 모든 특징에 대해 반복하며 가장 높은 정보 이득을 가진 특징을 기준으로 나무의 가지를 뻗어 나갑니다.
빅데이터 분석에서 **카이제곱 검정(Chi-square Test)**은 주로 **범주형 자료(Categorical Data)**를 다룰 때 사용하는 유용한 **특징선택(Feature Selection)** 기법입니다. 쉽게 말해, 특정 특징(변수)이 우리가 알고 싶어하는 결과(목표 변수)와 얼마나 관련이 있는지를 통계적으로 평가하는 방법이에요.

---

### 카이제곱 검정

카이제곱 검정은 두 개의 범주형 변수가 서로 **연관성(association)**이 있는지, 아니면 **독립적(independent)**인지를 확인하는 통계적 방법입니다.

예를 들어, "성별(남/여)"과 "구매한 물건(상품 A/상품 B)"이라는 두 가지 범주형 변수가 있다고 가정해 봅시다.
카이제곱 검정은 "성별에 따라 선호하는 상품이 다른가?"라는 질문에 대한 답을 찾는 데 사용될 수 있습니다.

* 만약 남성과 여성이 상품 A와 상품 B를 구매하는 비율이 같다면, 두 변수는 **독립적**이라고 볼 수 있어요.
* 만약 남성과 여성이 특정 상품을 구매하는 비율에 큰 차이가 있다면, 두 변수는 **연관성**이 있다고 볼 수 있습니다.

---

### 특징선택에서 어떻게 활용되나요?

빅데이터 분석에서는 수많은 특징(Feature)이 존재하는데, 이 모든 특징을 모델에 사용하는 것은 비효율적일 수 있습니다. 따라서 모델의 성능을 높이고 학습 속도를 빠르게 하기 위해 중요한 특징들만 골라내는 과정이 바로 특징선택입니다.

카이제곱 검정은 이 과정에서 다음과 같이 사용됩니다.

1.  **변수 간 연관성 측정:** 각 특징(예: 성별, 연령대, 거주 지역)이 우리가 예측하려는 목표 변수(예: 상품 구매 여부)와 얼마나 큰 연관성이 있는지를 **카이제곱 통계량**으로 계산합니다.
2.  **중요도 순위 매기기:** 카이제곱 통계량이 높을수록 두 변수 간의 연관성이 크다는 의미예요. 즉, 해당 특징이 목표 변수를 예측하는 데 더 유용하다는 뜻이죠.
3.  **최종 특징 선택:** 이렇게 계산된 통계량을 기준으로 특징들의 순위를 매기고, 상위 몇 개의 특징만 선택하여 모델을 만드는 데 활용합니다.

**쉽게 비유하자면,**
마케팅 데이터로 고객의 구매를 예측하는 모델을 만든다고 해볼게요. "성별", "선호하는 색상", "거주 지역" 등 수많은 특징이 있습니다. 이때, 카이제곱 검정은 각 특징(성별, 색상 등)이 '구매'라는 목표에 얼마나 중요한지를 점수로 매겨줍니다. 이 점수가 높은 특징일수록 구매를 예측하는 데 더 좋은 '단서'가 되는 것이죠.

### 카이제곱 검정 공식 예시

카이제곱 검정 ($$\chi^2$$) 공식은 **관측값(Observed)**과 **기대값(Expected)**의 차이를 이용합니다. 공식은 다음과 같습니다.

$$\chi^2 = \sum \frac{(O - E)^2}{E}$$

* **$$O$$ (Obs측정값):** 실제로 관찰된 값 또는 빈도.
* **$$E$$ (기대값):** 두 변수가 서로 관련이 없을 때(독립적일 때) 기대할 수 있는 값.
* **$$\sum$$ (시그마):** 모든 항목에 대한 계산값을 더하라는 의미.

---

### 간단한 예시: 광고 효과 검증

어떤 회사가 새로운 마케팅 광고가 특정 연령층에 더 효과적인지 알고 싶다고 가정해 봅시다. 100명의 사람을 대상으로 '광고를 본 후 상품을 구매했는지'에 대한 설문조사를 진행했습니다.

|            | 20대 | 30대 | 합계 |
| :--------- | :--- | :--- | :--- |
| **구매함** | 25   | 15   | 40   |
| **구매 안 함** | 25   | 35   | 60   |
| **합계** | 50   | 50   | 100  |

이 표는 **실제 관측값**($$O$$)입니다.

**1. 기대값 ($$E$$) 계산하기**

만약 연령대와 구매 여부가 아무런 관련이 없다면, 20대와 30대 모두 구매 비율은 전체 구매 비율과 같아야 합니다.

* **전체 구매 비율:** 전체 100명 중 40명이 구매했으므로 40%입니다.
* **전체 구매 안 한 비율:** 전체 100명 중 60명이 구매하지 않았으므로 60%입니다.

이 비율을 각 연령대별 합계에 적용하여 기대값을 계산합니다.

* **20대 중 구매를 기대하는 사람 ($$E_{20대-구매}$$):** 20대 합계(50명) × 전체 구매 비율(40%) = 20명
* **30대 중 구매를 기대하는 사람 ($$E_{30대-구매}$$):** 30대 합계(50명) × 전체 구매 비율(40%) = 20명
* **20대 중 구매를 안 할 것으로 기대하는 사람 ($$E_{20대-안함}$$):** 20대 합계(50명) × 전체 구매 안 한 비율(60%) = 30명
* **30대 중 구매를 안 할 것으로 기대하는 사람 ($$E_{30대-안함}$$):** 30대 합계(50명) × 전체 구매 안 한 비율(60%) = 30명

**2. 카이제곱 통계량 계산하기**

이제 각 항목의 **관측값($$O$$)**과 **기대값($$E$$)**을 공식에 넣어 계산합니다.

* **20대-구매:** $$\frac{(25 - 20)^2}{20} = \frac{5^2}{20} = \frac{25}{20} = 1.25$$
* **30대-구매:** $$\frac{(15 - 20)^2}{20} = \frac{(-5)^2}{20} = \frac{25}{20} = 1.25$$
* **20대-안함:** $$\frac{(25 - 30)^2}{30} = \frac{(-5)^2}{30} = \frac{25}{30} \approx 0.83$$
* **30대-안함:** $$\frac{(35 - 30)^2}{30} = \frac{5^2}{30} = \frac{25}{30} \approx 0.83$$

모든 계산값을 합산합니다.

$$\chi^2 = 1.25 + 1.25 + 0.83 + 0.83 = 4.16$$

**3. 결과 해석**

계산된 카이제곱 통계량($$4.16$$)을 **자유도**(degree of freedom)와 함께 통계표에 대조하거나, 통계 프로그램을 이용하여 **p-값**(p-value)을 확인합니다.

* **카이제곱 값이 크다는 것:** 관측값과 기대값의 차이가 크다는 의미로, 두 변수(연령대와 구매 여부) 사이에 **유의미한 연관성**이 있다는 뜻입니다.
* **카이제곱 값이 작다는 것:** 관측값과 기대값의 차이가 작다는 의미로, 두 변수가 **독립적**이라는 뜻입니다.

이 예시의 경우, 계산된 $$4.16$$이라는 값은 일반적으로 통계적으로 유의미한 것으로 간주되어 '연령대와 광고 효과 사이에 연관성이 있다'고 결론을 내릴 수 있습니다.  
즉, 이 광고는 20대에게 더 효과적이라고 해석할 수 있습니다.

### 피셔 스코어(Fisher Score)

피셔 스코어는 카이제곱 검정과 마찬가지로 **지도학습(Supervised Learning)** 기반의 **특징선택(Feature Selection)** 기법입니다.  
하지만 카이제곱 검정이 **범주형 변수**에 주로 사용되는 반면, 피셔 스코어는 **연속형 변수(Continuous Data)**에 사용됩니다.

피셔 스코어는 특정 특징(변수)이 우리가 구분하고 싶은 여러 **클래스(Class)**들을 얼마나 잘 **분리**해내는지 점수를 매깁니다.

쉽게 말해, "이 특징이 각 그룹을 얼마나 잘 나누는가?"를 평가하는 도구입니다.

### 피셔 스코어의 핵심 아이디어

피셔 스코어는 다음 두 가지 조건을 모두 만족하는 특징에 높은 점수를 줍니다.

1.  **클래스 내 분산(Within-class variance)이 작아야 합니다.**
    * 같은 그룹에 속한 데이터들끼리는 서로 뭉쳐 있어야 합니다.
    * 예: VIP 고객들의 '연간 구매액'은 서로 비슷한 범위에 있어야 합니다.

2.  **클래스 간 분산(Between-class variance)이 커야 합니다.**
    * 다른 그룹들 간에는 평균이 서로 멀리 떨어져 있어야 합니다.
    * 예: VIP 고객들의 '연간 구매액' 평균과 일반 고객들의 '연간 구매액' 평균이 크게 차이나야 합니다.

이 두 가지를 만족할수록 해당 특징은 클래스를 구분하는 데 매우 유용하다고 판단하고 높은 점수를 부여합니다.

### 피셔 스코어는 어떻게 계산되나요?

피셔 스코어는 기본적으로 다음의 비율을 계산하여 점수를 매깁니다.

$$\text{Fisher Score} = \frac{\text{클래스 간 분산}}{\text{클래스 내 분산}}$$

이때, **분자(numerator)가 크고 분모(denominator)가 작을수록** 피셔 스코어는 매우 높아집니다. 즉, 좋은 특징이라는 뜻입니다.

### 간단한 예시: 고객 분류

우리에게 **'고객 등급(VIP vs. 일반)'**이라는 클래스가 있고, 이를 예측하기 위한 두 가지 특징인 **'나이'**와 **'연간 구매액'**이 있다고 가정해 봅시다.

1.  **특징: 연간 구매액**
    * **클래스 내 분산:** VIP 고객들은 연간 구매액이 대부분 높고 서로 비슷한 수준일 것입니다. 일반 고객들도 마찬가지로 서로 비슷한 수준일 것입니다. (분모가 작음)
    * **클래스 간 분산:** VIP 고객들의 평균 연간 구매액은 일반 고객들의 평균 연간 구매액과 큰 차이를 보일 것입니다. (분자가 큼)
    * **결론:** 이 특징은 VIP와 일반 고객을 매우 잘 분리하므로 **피셔 스코어가 매우 높습니다.**

2.  **특징: 나이**
    * **클래스 내 분산:** VIP 고객들의 나이대가 매우 다양할 수 있습니다. 일반 고객들 역시 마찬가지입니다. (분모가 큼)
    * **클래스 간 분산:** VIP 고객들의 평균 나이와 일반 고객들의 평균 나이가 비슷할 수 있습니다. (분자가 작음)
    * **결론:** 이 특징은 고객 등급을 구분하는 데 유용하지 않으므로 **피셔 스코어가 낮습니다.**

이처럼 피셔 스코어는 각 특징의 점수를 계산하여 가장 높은 점수를 얻은 특징들을 선택함으로써 모델의 성능을 향상시키는 데 도움을 줍니다.

좋은 질문이에요 jong! 빅데이터 분석에서 **특징 선택(feature selection)**은 모델 성능을 높이고 계산 효율을 개선하는 핵심 과정입니다. 그중 **상관계수(Correlation Coefficient)**는 가장 직관적이고 널리 쓰이는 기법 중 하나예요. 아래에 자세히 설명해드릴게요.

---

## 📊 상관계수를 활용한 특징 선택

### ✅ 개념

**상관계수**는 두 변수 간의 **선형 관계의 강도와 방향**을 나타내는 값입니다.  
특징 선택에서는 **입력 변수(특징)**와 **타겟 변수(예측 대상)** 간의 상관성을 측정하여,  
**예측에 유의미한 영향을 주는 변수만 선택**하는 데 사용됩니다.

---

### 📐 대표적인 상관계수 종류

| 종류 | 설명 | 값의 범위 | 해석 |
|------|------|------------|------|
| **피어슨 상관계수** | 선형 관계 측정 | \(-1\) ~ \(+1\) | ±1에 가까울수록 강한 선형 관계 |
| **스피어만 상관계수** | 순위 기반 비선형 관계 | \(-1\) ~ \(+1\) | 순위가 비슷할수록 높은 상관 |
| **켄달의 타우** | 순위 일관성 측정 | \(-1\) ~ \(+1\) | 순위 간 일관성 높을수록 높은 상관 |

> 💡 대부분의 경우 **피어슨 상관계수**를 사용합니다.

---

### 📈 피어슨 상관계수 공식

$$
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
$$

- $$\( x_i \)$$: 입력 변수 값  
- $$\( y_i \)$$: 타겟 변수 값  
- $$\( \bar{x}, \bar{y} \)$$: 각각의 평균

---

### 🧪 예시: 상관계수 기반 특징 선택

```python
import pandas as pd

# 예시 데이터
df = pd.DataFrame({
    'feature1': [1, 2, 3, 4, 5],
    'feature2': [5, 4, 3, 2, 1],
    'feature3': [2, 2, 2, 2, 2],
    'target':    [1, 2, 3, 4, 5]
})

# 피어슨 상관계수 계산
correlations = df.corr()['target'].drop('target')
selected_features = correlations[abs(correlations) > 0.5].index.tolist()

print("선택된 특징:", selected_features)
```

✅ 결과:
- `feature1`: 상관계수 +1 → 강한 양의 상관 → 선택됨
- `feature2`: 상관계수 −1 → 강한 음의 상관 → 선택됨
- `feature3`: 상관계수 0 → 상관 없음 → 제거됨

---

## 🎯 특징 선택 기준

- **절대값 기준**: 일반적으로 `|r| > 0.5` 또는 `|r| > 0.7` 이상이면 유의미한 변수로 간주
- **다중공선성 제거**: 입력 변수들끼리 상관이 너무 높으면 하나를 제거 (예: `feature1`과 `feature2`가 서로 −1)

---

## ⚠️ 주의할 점

- **선형 관계만 측정** → 비선형 관계는 놓칠 수 있음
- **타겟 변수가 범주형일 경우** → 피어슨 대신 ANOVA, 카이제곱 등을 사용해야 함
- **다중공선성 문제** → 상관이 너무 높은 변수는 모델에 부정적 영향

---

