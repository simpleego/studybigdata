

-----

## **Chapter 01: 빅데이터의 이해** 

### **1. 빅데이터 개요 및 활용** 

#### **(1) 빅데이터 특징** 

**① 빅데이터(Big Data) 개념** 

  * 빅데이터는 막대한 양(수십 테라바이트 이상)의 정형 및 비정형 데이터입니다. 
  * 데이터로부터 가치를 추출하고 결과를 분석하여 통찰, 지혜를 얻는 과정입니다. 
  * Ackoff, R.L이 도식화한 DIKW 피라미드로 표현할 수 있습니다.

    

**DIKW 피라미드** 
<img width="850" height="563" alt="image" src="https://github.com/user-attachments/assets/049cf8de-774f-4113-9b4e-0f1ab931c714" />

 

| 피라미드 요소 | 설명 |
| :--- | :--- |
| **데이터 (Data)** | 객관적 사실로서 다른 데이터와의 상관관계가 없는 가공하기 전의 순수한 수치나 기호.  예: 수제비 빅분기 책을 A 사이트에서 30,000원, B 사이트에서 35,000원에 판매.  |
| **정보 (Information)** | 가공, 처리하여 데이터 간의 연관 관계와 함께 의미가 도출된 요소.  예: 수제비 빅분기, 정처기 책은 A 사이트에서 더 싸게 판매.  |
| **지식 (Knowledge)** | 획득된 다양한 정보를 구조화하여 유의미한 정보로 분류하고 일반화시킨 결과물.  예: 서적들은 A 사이트가 싸게 팔기 때문에 수제비 책을 구입할 계획.  |
| **지혜 (Wisdom)** | 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적 아이디어.  예: A 사이트의 다른 상품들도 B 사이트보다 저렴할 것으로 판단.  |

  * 기존의 관리 방법으로는 처리하기 어려운 막대한 양의 데이터를 처리할 때 빅데이터를 사용합니다. 

**② 데이터의 양을 측정하는 단위** 

| 기호 | 이름 | 값 |
| :--- | :--- | :--- |
| KB | 킬로바이트 | $1KB=10^{3}Bytes$  |
| MB | 메가바이트 | $1MB=10^{3}KB=10^{6}Bytes$  |
| GB | 기가바이트 | $1GB=10^{3}MB=10^{9}Bytes$  |
| TB | 테라바이트 | $1TB=10^{3}GB=10^{12}Bytes$  |
| PB | 페타바이트 | $1PB=10^{3}TB=10^{15}Bytes$  |
| EB | 엑사바이트 | $1EB=10^{3}PB=10^{18}Bytes$  |
| ZB | 제타바이트 | $1ZB=10^{3}EB=10^{21}Bytes$  |
| YB | 요타바이트 | $1YB=10^{3}ZB=10^{24}Bytes$  |

-----

#### **(2) 빅데이터 특성** 

빅데이터는 전통적으로 3V (Volume, Variety, Velocity)의 특성이 있으며, 최근에는 4V (Value 추가), 5V (Veracity 추가), 7V (Validity, Volatility 추가)로 확장되고 있습니다. 

|         **특성**        |                                                           **설명**                                                           |
|:-----------------------:|----------------------------------------------------------------------------------------------------------------------------|
| **규모 (Volume)**       | 빅데이터 분석 규모와 관련된 특징으로, ICT 기술 발전으로 디지털 정보량이 기하급수적으로 증가하는 것을 의미합니다.             |
| **다양성 (Variety)**    | 빅데이터 자원 유형에 관련된 특징으로, 정형 데이터뿐만 아니라 비정형, 반정형 데이터를 포함합니다.                             |
| **속도 (Velocity)**     | 빅데이터 수집·분석·활용 속도에 관련된 특징으로, 실시간성 정보의 생성 속도 증가에 따라 처리 속도 가속화가 요구됩니다.         |
| **가치 (Value)**        | 빅데이터 수집 데이터를 통해 얻을 수 있는 가치로, 비즈니스나 연구에 활용되어 유용한 가치를 끌어낼 수 있는지를 의미합니다.     |
| **신뢰성 (Veracity)**   | 빅데이터 수집 대상 데이터가 가지는 신뢰에 관련된 특징으로, 노이즈 및 오류 제거를 통해 데이터 품질과 신뢰성을 높여야 합니다.  |
| **정확성 (Validity)**   | 빅데이터 수집 대상 데이터가 가지는 유효성과 정확성으로, 질 높은 데이터를 활용한 정확한 분석이 중요합니다.                    |
| **휘발성 (Volatility)** | 빅데이터 수집 대상 데이터가 의미를 가지는 기간으로, 데이터가 얼마나 오래 저장되고 타당하게 쓰일 수 있는지에 관한 사항입니다. |
-----

#### **(3) 데이터 지식경영** 

  * 빅데이터는 장기적인 관점에서 유용한 가치를 창출해야 합니다. 
  * 데이터 기반 지식경영의 핵심은 암묵지와 형식지의 상호작용에 있습니다. 

**지식 구분** 

| 구분 | 설명 |
| :--- | :--- |
| **암묵지 (Tacit Knowledge)** | 학습과 경험을 통해 개인에게 체화되어 있지만 겉으로 드러나지 않는 지식입니다.  예: 수영, 태권도.  |
| **형식지 (Explicit Knowledge)** | 문서나 매뉴얼처럼 형상화된 지식으로 전달과 공유가 용이합니다.  예: 수험서, 소프트웨어 설치 매뉴얼.  |

---
<img width="704" height="650" alt="image" src="https://github.com/user-attachments/assets/ea8c3ec5-7e76-426c-bbc5-866590dda2c0" />


**데이터 지식경영 상호작용 (SECI 모델)** 
 

| 상호작용 | 내용 |
| :--- | :--- |
| **공통화 (Socialization)** | 다른 사람과의 상호작용을 통해 개인이 암묵지를 습득하는 단계.  |
| **표출화 (Externalization)** | 개인에게 내재된 경험을 객관적인 데이터(문서, 매체)로 저장하거나 가공, 분석하는 과정.  |
| **연결화 (Combination)** | 형식지가 상호 결합하여 새로운 형식지를 창출하는 과정.  |
| **내면화 (Internalization)** | 행동과 실천교육 등을 통해 형식지가 개인의 암묵지로 체화되는 단계.  |

-----

## **4 빅데이터 가치**
###  **(1) 빅데이터 가치**

▣ 빅데이터 가치

| 가 치             | 설 명                                                                                                                                     |
|------------------|------------------------------------------------------------------------------------------------------------------------------------------|
| **경제적 자산**      | 새로운 기회를 창출하고, 위험을 해결하여 사회 및 경제 발전의 엔진 역할을 수행                                                           |
| **불확실성 제거**    | - 사회현상, 현실 세계의 데이터를 기반으로 한 패턴 분석과 미래 전망<br>- 여러 가지 가능성에 대한 시나리오 시뮬레이션                    |
| **리스크 감소**      | - 환경, 소셜 네트워크, 모니터링 정보의 패턴 분석을 통해 위험 징후 및 이상 신호 포착<br>- 이슈를 사전에 인지 및 분석하고 빠른 의사결정과 실시간 대응 |
| **스마트한 경쟁력**  | - 대규모 데이터 분석을 통한 상황 인지, 인공지능 서비스 기능<br>- 개인화, 자동화 서비스 제공 확대<br>- 트렌드 변화 분석을 통한 제품 경쟁력 확보 |
| **타 분야 융합**     | - 타 분야와의 융합을 통한 새로운 가치 창출<br>- 방대한 데이터 활용을 통한 새로운 융합시장 창출                                         |


**(2) 빅데이터 가치 산정이 어려운 이유** 
데이터 활용 방식의 다양화, 새로운 가치 창출, 분석기술의 발전으로 인해 빅데이터의 가치를 정확하게 산정하기 어렵습니다. 

▣ 빅데이터 가치 산정이 어려운 이유
| 원인                     | 설명                                                                                                                                                                                                 |
|--------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 데이터 활용 방식의 다양화 | • 데이터의 재사용, 데이터의 재조합, 다목적용 데이터 개발 등이 일반화되면서 특정 데이터를 언제/어디서/누가 활용할지 알 수 없어서 가치 산정이 어려움<br>• 데이터의 창의적 조합으로 인해 기존에 풀 수 없는 문제를 해결하는 데 도움을 주기 때문에 가치 산정이 어려움<br>**예)** 구글이 검색 결과를 낼 때마다 구글은 클라우드에 저장된 웹 사이트 정보를 매번 사용 |
| 새로운 가치 창출         | • 빅데이터 시대에 데이터가 기존에 없던 가치를 창출하여 가치 산정이 어려움<br>**예)** 고객의 성향을 분석하여 고객 맞춤 서비스 제공                                                                 |
| 분석기술의 급속한 발전   | • 비용 문제로 인해 분석할 수 없었던 것을 저렴한 비용으로 분석하면서 활용도가 증가하여 가치 산정이 어려움<br>**예)** 텍스트 마이닝을 통한 SNS 분석                                                  |


##   (3) 빅데이터 영향  
빅데이터의 가치를 활용함으로써 기업, 정부, 개인이 스마트해지고 있습니다. 

▣ 빅데이터 영향
| 대   상 | 영  향 | 설  명 |
| :--- | :--- | :--- |
| **기 업** | 혁신 수단 제공, 경쟁력 강화, 생산성 향상  | 소비자의 행동 분석, 시장 변동 예측을 통해 비즈니스 모델을 혁신하거나 신사업을 발굴합니다.|
| **정 부** | 환경 탐색, 상황 분석, 미래 대응 가능  | 사회 변화를 추정하고 각종 재해 관련 정보를 추출하며 미래 의제를 도출합니다.  |
| **개 인** | 목적에 따른 활용, 적시에 필요한 정보 획득  | 빅데이터 서비스를 저렴한 비용으로 활용하여 필요한 정보를 얻습니다.  |

** (4) 빅데이터 위기 요인 및 통제 방안** 
빅데이터는 유용한 가치를 주는 동시에 사생활 침해, 책임 원칙 훼손, 데이터 오용과 같은 부정적인 영향을 줄 수 있어 통제 방안이 필요합니다. 

### ① 빅데이터 위기 요인

▣ 빅데이터 위기요인
| 위기 요인         | 설명 |
|------------------|------|
| 사생활 침해| • 목적 외로 활용된 개인정보가 포함된 데이터의 경우 사생활 침해를 넘어 사회·경제적 위험으로 확대<br>• 여행 사실을 페이스북에 올린 사람의 집을 도둑이 노리는 사례 발생 |
| 책임 원칙 훼손     | • 예측 기술과 빅데이터 분석기술이 발전하면서 분석 대상이 되는 사람들이 예측 알고리즘의 희생양이 될 가능성도 증가<br>• 잠재적 위험이 아닌 명확한 결과에 대한 책임을 묻고 있는 민주주의 국가의 원리를 훼손할 가능성이 존재<br>• 범죄 예측 프로그램에 의해 범행을 저지르기 전에 체포, 자신의 신용도와 무관하게 부당하게 대출 거부 |
| 데이터 오용  ______| • 데이터 분석은 실제 일어난 일에 대한 데이터에 의존하기 때문에 이를 바탕으로 미래를 예측하는 것은 언제나 맞을 수는 없는 오류가 존재함<br>• 잘못된 지표를 사용하는 것도 빅데이터의 피해가 될 수 있음 |


### ② 빅데이터 위기 요인 및 통제 방안
▣ 빅데이터 위기 요인 및 통제 방안
<table border="1" cellspacing="0" cellpadding="8">
  <thead  style="background-color: gray;" >
    <tr>
      <th style="width: 100px;">통제 방안</th>
      <th style="width: 200px;">위기 요인</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>책임의 강조</td>
      <td>사생활 침해</td>
      <td>
        • 빅데이터를 통한 개인정보 침해 문제 해결을 위해 개인정보를 사용하는 사용자의 '책임'을 통해 해결하는 방안 강조<br>
        • 사용자에게 개인정보의 유출 및 동의 없는 사용으로 발생하는 피해에 대한 책임을 지게 함으로써 사용 주체가 적극적인 보호 장치를 마련할 수 있도록 함
      </td>
    </tr>
    <tr>
      <td>결과 기반의 책임 적용</td>
      <td>책임 원칙 훼손</td>
      <td>
        • 책임의 강조를 위해서는 기존의 원칙 보강 및 강화와 예측 자료에 의한 불이익 가능성을 최소화하는 장치를 마련하는 것이 필요<br>
        • 판단을 근거로 오류가 있는 예측 알고리즘을 통해서는 불이익을 줄 수 없으며, 방지를 위한 피해 최소화 장치 마련 필요
      </td>
    </tr>
    <tr>
      <td>알고리즘에 대한 접근 허용</td>
      <td>데이터 오용</td>
      <td>
        • 예측 알고리즘의 부당함을 반증할 수 있는 '알고리즘에 대한 접근권'을 제공<br>
        • 알고리즘을 통해 불이익을 당한 사람들을 대변할 알고리즘미스트라는 전문가가 필요
      </td>
    </tr>
  </tbody>
</table>


## ** (5) 분석 가치 에스컬레이터 (Analytic Value Escalator) ** 
- 분석 가치 에스컬레이터는 가트너가 빅데이터의 가치를 묘사(Descriptive) 분석, 진단(Diagnostic) 분석, 예측(Predictive) 분석, 처방(Prescriptive) 분석의 4단계로 정의한 개념이다.
- 분석 가치 에스컬레이터에서는 높은 난이도를 수반하는 데이터 분석은 더 많은 가치를 창출한다.

<img width="1107" height="799" alt="image" src="https://github.com/user-attachments/assets/5d42fcae-1d34-4b29-ac82-5f246bec7f5c" />

--- 
▣ 가트너늬 분석가치 에스컬레이터
<table border="1" cellspacing="0" cellpadding="8">
  <thead>
    <tr>
      <th style="width: 100px">순 서</th>
      <th>단 계</th>
      <th>설 명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>묘사 분석<br> (Descriptive Analysis)</td>
      <td>
        • 분석의 가장 기본적인 지표를 확인하는 단계<br>
        • 과거에 어떤 일이 일어났고, 현재는 무슨 일이 일어나고 있는지 확인<br>
         <b>예: 단순한 소비자 선호도(좋다, 나쁘다)뿐만 아니라 선호하는 대상까지 확인<b>
      </td>
    </tr>
    <tr>
      <td>2</td>
      <td>진단 분석<br> (Diagnostic Analysis)</td>
      <td>
        • 묘사 단계에서 찾아낸 분석의 원인을 이해하는 단계<br>
        • 데이터를 기반으로 왜 발생했는지 이유를 확인<br>
        <b> 예: 분기별로 매출 차이가 발생한 이유 확인<b>
      </td>
    </tr>
    <tr>
      <td>3</td>
      <td>예측 분석 <br>(Predictive Analysis)</td>
      <td>
        • 데이터를 통해 기업 혹은 조직의 미래, 고객의 행동 등을 예측하는 단계<br>
        • 무슨 일이 일어날 것인지를 예측<br>
        <b> 예: 패턴을 분석하여 고객 이탈 가능성 확인, 고객 구매 이력으로 상품 추천<b>
      </td>
    </tr>
    <tr>
      <td>4</td>
      <td>처방 분석<br> (Prescriptive Analysis)</td>
      <td>
        • 예측을 바탕으로 최적화하는 단계<br>
        • 무엇을 해야 할 것인지를 확인<br>
        <b> 예: 고객 이탈을 막을 수 있는 최적 시점 탐색, 투자 대비 최적 수익률을 보장하는 종목 추천<b>
      </td>
    </tr>
  </tbody>
</table>

-----
## 3)  빅데이터 산업의 이해 

## (1) 빅데이터 산업 개요

- 스마트폰, SNS, 사물인터넷(IoT) 확산 등에 따라 데이터 활용이 증가하여 **빅데이터는 신성장동력**으로 급부상하고 있다.
- 클라우드 컴퓨팅 기술의 발전으로 **데이터 처리 비용이 급격하게 감소**하여 빅데이터가 발전하고 있다.
- 주요국 및 글로벌 기업은 빅데이터 **'산업' 육성 및 '활용'**에 주력하고 있다.
- 우리나라는 데이터 생산량이 많은 산업(통신·제조업 등)이 발달해 **잠재력이 크지만**, 불확실성에 따른 투자 리스크 등으로 **'활용'은 저조**하다.



## (2) 산업별 빅데이터 활용

▣ 산업별 빅데이터 활용

<table border="1" cellspacing="0" cellpadding="8">
  <thead>
    <tr>
      <th>산업</th>
      <th>활용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>의료·건강</td>
      <td>
        • 헬스케어 플랫폼 등을 통한 개인 건강정보의 축적 및<br>
        • 의료기관 등과 공유·활용
      </td>
    </tr>
    <tr>
      <td>과학기술</td>
      <td>
        • 주요 분야의 연구·개발 성과물을 바탕으로<br>
        • 대규모 과학기술 빅데이터 공유·활용 플랫폼 구축
      </td>
    </tr>
    <tr>
      <td>정보보안</td>
      <td>
        • 빅데이터 분석을 통해 해킹 등의 보안사고 징후 파악<br>
        • 조기 대응 및 협업시스템 구축
      </td>
    </tr>
    <tr>
      <td>제조·공정</td>
      <td>
        • 완제품 품질향상 등을 위해 대기업이 빅데이터 시스템 구축 및 납품<br>
        • 중소·중견기업이 공동으로 활용 (정부 예산 투자 또는 필요 SW 개발 등 지원)
      </td>
    </tr>
    <tr>
      <td>소비·가계</td>
      <td>
        • 구매 패턴 및 트랜잭션 분석을 통한 소비 트렌드 예측<br>
        • 시뮬레이션을 통한 판매 포트폴리오 구성 지원 및 리스크 관리
      </td>
    </tr>
    <tr>
      <td>교통·물류</td>
      <td>
        • 수요예측 및 제어를 통한 물류·유통체계 최적화
      </td>
    </tr>
  </tbody>
</table>



### **4) 빅데이터  조직 및 인력 **

#### **(1) 빅데이터 조직 설계** 

빅데이터 서비스 도입 및 운영 조직을 구성하기 위해서는  **빅데이터 업무 프로세스를 이해하고, 조직의 특성을 고려하여야 한다.**

---

## ① 빅데이터 업무 프로세스

- **빅데이터 도입 및 운영**은 다음과 같은 단계로 진행된다:
  1. **빅데이터 도입 계획 수립**
  2. **빅데이터 시스템 구축**
  3. **빅데이터 서비스 운영**


**① 빅데이터 업무 프로세스** 
빅데이터 도입 및 운영은 도입 기획, 시스템 구축, 서비스 운영의 단계로 진행됩니다. 

| 단계 | 설명 |
| :--- | :--- |
| **빅데이터 도입 단계**  | 빅데이터 시스템 구축을 위한 기획, 기술 검토, 조직 구성, 예산 확보 등을 수행합니다.  |
| **빅데이터 구축 단계**  | 요구사항 분석, 설계, 구현, 테스트를 통해 빅데이터 플랫폼을 구축합니다.  |
| **빅데이터 운영 단계**  | 구축된 시스템을 인수하여 운영 계획을 수립하고, 플랫폼, 데이터, 분석 모델을 운영합니다.  |

## **② 조직 구조 설계 요소** 
### ⓐ 조직구조 설계의 요소
조직의 목적을 성공적으로 달성하기 위하여 업무 활동, 부서화, 보고 체계를 고려한다.

**▣ 조직구조 설계요소**
<table><thead>
  <tr>
    <th>요소</th>
    <th colspan="2">설명</th>
  </tr></thead>
<tbody>
  <tr>
    <td rowspan="3">업무 활동</td>
    <td colspan="2">         • 조직의 미션과 목적을 달성하기 위하여 과업 수행을 위해<br>         • 수적 업무 활동과 수행 업무 활동으로 구분       </td>
  </tr>
  <tr>
    <td colspan="2">         수적 업무 활동<br>         • 경영 계획, 예산 할당 등 우선순위를 결정       </td>
  </tr>
  <tr>
    <td colspan="2">         수행 업무 활동<br>         • 업무 프로세스 절차별로 업무를 배분       </td>
  </tr>
  <tr>
    <td>부서화</td>
    <td colspan="2">         • 조직의 미션과 목적을 효율적으로 달성하기 위한 조직 구조 유형 설계<br>         • 조직 구조 유형은 집중 구조, 기능 구조, 분산 구조로 분류       </td>
  </tr>
  <tr>
    <td>보고 체계</td>
    <td colspan="2">         • 조직의 목표 달성을 위하여 업무 활동 및 부서의 보고 체계를 설계       </td>
  </tr>
</tbody>
</table>

### ⓑ 조직구조 유형 
- 빅데이터 조직 구조는 집중 구조, 기능 구조, 분산 구조로 나뉩니다. 

▣ 빅데이터 조직 구조 유형
<img width="1038" height="352" alt="image" src="https://github.com/user-attachments/assets/e13c4c2b-fed3-4c27-9a12-7ccf55709b4f" />

<img width="1038" height="647" alt="image" src="https://github.com/user-attachments/assets/fad87b15-d80d-4077-b411-37fd9b58aa5b" />


-----

## ③ 조직 구조의 설계 특성

조직 구조를 설계할 때는 다음과 같은 특성을 고려한다:

- **공정화**
- **분업화**
- **직무 전문성**
- **통제 범위**
- **의사소통 및 조정**

▣ 조직 구조의 설계 특성
<table border="1">
  <thead>
    <tr>
      <th>특성</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>공식화 (Formalization)</td>
      <td>업무의 수행 절차, 수행 방법, 작업 결과 등의 기준을 사전에 설정하여 공식화</td>
    </tr>
    <tr>
      <td>분업화 (Division of Labor)</td>
      <td>업무의 성격에 따라 여러 단위로 나누는 수평적 분할과 계획, 감독, 실무 업무 실행 등의 수준에 따라 나누는 수직적 분할로 구분</td>
    </tr>
    <tr>
      <td>직무 전문화 (Job Specialization)</td>
      <td>직무 전문화는 수행 업무에 활용되는 직무 전문성의 유형을 의미하며, 직무 전문성에 따라 생산성이 증대되므로 전문 지식과 경험이 중요한 요소</td>
    </tr>
    <tr>
      <td>통제 범위 (Span of Control)</td>
      <td>관리자가 효율적이며 효과적으로 관리할 수 있는 조직의 인원수</td>
    </tr>
    <tr>
      <td>의사소통 및 조정 (Communication and Coordination)</td>
      <td>업무 수행 시 의사소통은 업무의 지시, 보고, 피드백 등 수직적인 활동과 문제 해결을 위한 협업 등 수평적인 활동으로 구분</td>
    </tr>
  </tbody>
</table>
--- 

###  **(2) 조직 역량**

### ① 조직 역량 개념

- 조직 역량은 조직 구성원의 역량을 확보하며 조직 구성원들이 조직이 기대한 성과를 낼 수 있도록 하는 중요한 요소이다.
- 기업이나 조직을 지속적으로 경영하기 위해서는 조직 역량의 확보가 필수적이다.

## 조직 역할 모델링

- 조직 목표 달성을 위해서는 상위 성과자의 기여가 중요하다.
- 상위 성과자의 행동 특성을 이해하고 이를 공유함으로써 조직 목표를 달성할 수 있다.
- 직무 수행에 필요한 지식, 기술, 태도를 기반으로 직무 역할 모델을 설계한다.
- 데이터 과학자에게 요구되는 역량은 소프트 스킬과 하드 스킬로 나뉜다.

---

## 데이터 과학자 요구역량

▣ 데이터 사이언티스트 요구 역량
| 역량               | 설명                                           |
|--------------------|------------------------------------------------|
| 소프트 스킬 (Soft Skill) | 모든 직무에서 사용할 수 있는 기술                  |
| 하드 스킬 (Hard Skill)   | 해당 업무를 수행하기 위해 필요한 실질적인 기술     |


**▣ 데이터 사이언티스트 요구 역량 상세**

| 구분 | 스킬 | 설명 |
| :--- | :--- | :--- |
| **소프트 스킬 (Soft Skill)**  | 협력 능력, 통찰력, 전달력 | 커뮤니케이션, 논리적 비판, 창의적 사고, 스토리텔링 능력 등을 포함합니다.  |
| **하드 스킬 (Hard Skill)**  | 분석기술의 숙련도, 이론적 지식 | 최적 분석 설계, 빅데이터 관련 기법 및 방법론 습득 등 실질적인 기술을 포함합니다.  |

## ③ 데이터 분야 직무별 업무

# 데이터 직무별 업무 내용

| 구분 | 직무명 | 업무 내용 |
|------|--------|-----------|
| 데이터 엔지니어<br>(Data Engineer) | • 조직 전체에서 빅데이터를 수집, 저장, 처리, 분석이 가능하도록 데이터 파이프라인과 인프라를 구축하고 운영하는 업무<br>• 다양한 소스에서 오는 대용량의 정형 및 비정형 데이터를 수집하고, 이를 정제, 변환, 적재하는 ETL(Extract, Transform, Load) 프로세스를 설계하고 구현하는 업무<br>• 실시간 데이터 스트리밍 처리를 위한 시스템을 구축하고 관리하는 업무<br>• 데이터의 품질을 보장하고, 데이터 거버넌스 정책을 수립하여 데이터의 일관성과 신뢰성을 유지하는 업무 |
| 데이터 분석가<br>(Data Analyst) | • 최적의 의사결정을 내리는 데 도움을 주는 비즈니스 인사이트를 제공하는 업무<br>• 데이터의 경향, 패턴, 이상값 등을 인식하기 위한 시각화 작업 및 보고서 작성 업무 |
| 데이터 사이언티스트<br>(Data Scientist) | • 비즈니스 탐과 연계해 각 팀의 전략을 수립하거나 업무 효율화에 필요한 데이터를 수집하고 분석하는 업무<br>• 머신러닝 모델을 사용해 정형, 비정형 데이터에서 인사이트를 창출하는 업무<br>• 사내 데이터를 이용해서 고객 행동 패턴 모델링 전략, 패턴을 찾아내거나 이상값을 탐지하는 업무<br>• 예측 모델링, 추천 시스템 등을 개발해 비즈니스 의사결정에 필요한 인사이트를 제공하는 업무 |
| 데이터 아키텍트<br>(Data Architect) | • 데이터베이스를 쉽게 동합, 중앙 집중화 및 보호할 수 있도록 기업의 데이터 관리를 위한 청사진을 만드는 업무<br>• 효율성 및 보안을 고려한 기업의 데이터 아키텍트 계획 및 관리 업무<br>• 기업의 데이터를 정형 데이터베이스에서 하둡(Hadoop) 기반의 비정형 데이터 베이스로 이관하려고 할 때 아파 프로세스 정립, 모니터링, 테스트를 주도 |


#### **(3) 데이터 거버넌스 (Data Governance)** 

데이터 거버넌스는 기업에서 사용하는 데이터의 가용성, 유용성, 통합성, 보안성을 관리하기 위한 정책과 프로세스입니다. 

**구성요소** 

  * **원칙 (Principle)**: 데이터를 유지·관리하기 위한 지침과 가이드. 
  * **조직 (Organization)**: 데이터를 관리할 조직의 역할과 책임(R\&R). 
  * **프로세스 (Process)**: 데이터 관리를 위한 활동과 체계. 

-----

#### **(4) 분석 준비도(Readiness) 및 성숙도(Maturity)** 

**① 데이터 분석 준비도 프레임워크** 
기업의 데이터 분석 도입 수준을 파악하기 위한 진단 방법으로 6가지 영역이 있습니다. 

  * 분석 업무 파악 
  * 인력 및 조직 
  * 분석 기법 
  * 분석 데이터 
  * 분석 문화 
  * IT 인프라 

**② 조직평가를 위한 성숙도 단계** 
기업의 분석 수준을 평가하는 4단계 모델입니다. 

| 단계 | 설명 |
| :--- | :--- |
| **도입 단계**  | 분석을 시작해 환경과 시스템을 구축하는 단계.  |
| **활용 단계**  | 분석 결과를 실제 업무에 적용하는 단계.  |
| **확산 단계**  | 전사 차원에서 분석을 관리하고 공유하는 단계.  |
| **최적화 단계**  | 분석을 진화시켜 혁신 및 성과 향상에 기여하는 단계.  |

**③ 사분면 분석 (Analytics Quadrant)** 
성숙도와 준비도에 따라 기업을 4가지 유형으로 구분하여 개선 방안을 수립합니다. 
 

| 유형 | 설명 |
| :--- | :--- |
| **준비형** | 낮은 준비도와 낮은 성숙도 수준. 사전 준비가 필요한 기업.  |
| **정착형** | 준비도는 낮으나, 제한적으로 분석을 사용하고 있어 정착이 필요한 기업.  |
| **도입형** | 분석 기법 등은 부족하지만 준비도가 높아 바로 도입할 수 있는 기업.  |
| **확산형** | 필요한 구성요소를 갖추고 있어 지속적인 확산이 필요한 기업.  |

-----

## **Chapter 02: 데이터 분석 계획** 

### **1. 분석 방안 수립** 

#### **(1) 하향식 접근 방식 (Top Down Approach)** 

  * **개념**: 분석 과제가 이미 정해져 있고, 이에 대한 해법을 찾기 위해 체계적으로 분석하는 방법입니다. 
  * **절차**: 문제 탐색 → 문제 정의 → 해결방안 탐색 → 타당성 검토 → 선택의 과정을 거칩니다. 

| 단계 | 설명 |
| :--- | :--- |
| **1. 문제 탐색 (Problem Discovery)**  | 비즈니스 모델을 기반으로 문제를 탐색하고 분석 기회를 발굴합니다.  |
| **2. 문제 정의 (Problem Definition)**  | 비즈니스 문제를 데이터 문제로 변환하여 정의하고, 필요한 데이터 및 기법을 정의합니다.  |
| **3. 해결방안 탐색 (Solution Search)**  | 정의된 문제를 해결하기 위한 다양한 분석 방안을 탐색합니다.  |
| **4. 타당성 검토 (Feasibility Study)**  | 경제적, 데이터 및 기술적 타당성을 평가합니다.  |
| **5. 선택 (Selection)**  | 타당성에 입각하여 최적의 대안을 선택하고 프로젝트화합니다.  |

-----

#### **(2) 상향식 접근 방식 (Bottom Up Approach)** 

  * **개념**: 문제 정의가 어려운 경우, 데이터를 기반으로 문제를 지속적으로 개선하는 방식입니다.  디자인 사고(Design Thinking) 접근법을 사용합니다. 
  * **절차**: 프로세스 분류 → 프로세스 흐름 분석 → 분석 요건 식별 → 분석 요건 정의의 과정을 거칩니다. 

-----

#### **(3) 대상별 분석 기획 유형** 

분석의 대상(What)과 방법(How)을 아는지 모르는지에 따라 4가지 유형으로 분류됩니다. 
 

| 유형 | 대상(What) | 방법(How) | 설명 |
| :--- | :--- | :--- | :--- |
| **최적화 (Optimization)**  | Known | Known | 대상과 방법을 모두 알고 있을 때, 개선을 통해 최적화를 수행합니다.  |
| **솔루션 (Solution)**  | Known | Un-Known | 대상은 알지만 방법을 모를 때, 해당 분석 주제에 대한 솔루션을 찾아냅니다.  |
| **통찰 (Insight)**  | Un-Known | Known | 방법은 알지만 대상이 명확하지 않을 때, 새로운 지식(통찰)을 도출합니다.  |
| **발견 (Discovery)**  | Un-Known | Un-Known | 대상과 방법을 모두 모를 때, 분석 대상 자체를 새롭게 도출합니다.  |

-----

#### **(4) 분석 과제 우선순위 평가** 

정의된 데이터 과제의 실행 순서를 정하기 위해 **시급성**과 **난이도**를 기준으로 평가합니다.  시급성은 ROI 관점의 비즈니스 효과와, 난이도는 투자 비용 요소와 관련이 깊습니다. 

 

  * **가장 우선순위가 높은 영역**: **III 사분면** (시급성: 현재, 난이도: 쉬움) 
  * **적용 우선순위 (시급성 기준)**: III → I → IV → II 
  * **적용 우선순위 (난이도 기준)**: III → IV → I → II 

-----

#### **(5) 빅데이터 분석 방법론** 

**① KDD 분석 방법론** 

  * **개념**: 1996년 Fayyad가 프로파일링 기술을 기반으로 통계적 패턴이나 지식을 찾기 위해 정리한 방법론입니다. 
  * **절차**: 데이터 세트 선택 → 데이터 전처리 → 데이터 변환 → 데이터 마이닝 → 데이터 마이닝 결과 평가 
   

**② CRISP-DM 분석 방법론** 

  * **개념**: 비즈니스 이해를 바탕으로 데이터 분석 목적의 6단계로 진행되는 데이터 마이닝 방법론입니다. 
  * **절차**: 업무 이해 → 데이터 이해 → 데이터 준비 → 모델링 → 평가 → 전개. 단계 간 피드백을 통해 완성도를 높입니다. 
   

**③ SEMMA 분석 방법론** 

  * **개념**: SAS사가 주도한 통계 중심의 5단계 방법론입니다. 
  * **절차**: 샘플링(Sampling) → 탐색(Exploration) → 수정(Modification) → 모델링(Modeling) → 검증(Assessment) 

-----

## **Chapter 03: 데이터 수집 및 저장 계획** 

### **1. 데이터 수집 및 전환** 

#### **(1) 데이터 수집** 

수집 데이터는 위치에 따라 **내부 데이터**와 **외부 데이터**로 구분됩니다. 

| 유형 | 설명 | 예시 |
| :--- | :--- | :--- |
| **내부 데이터**  | 조직 내부에 위치하며, 주로 정형 데이터입니다.  | ERP, CRM, SCM 데이터, VOC 데이터  |
| **외부 데이터**  | 조직 외부에 위치하며, 주로 비정형 데이터입니다.  | 소셜 데이터(SNS), 공공 데이터(LOD), 센서 데이터  |

#### **(2) 데이터 수집 방식 및 기술** 

  * **ETL (Extract Transform Load)**: 다양한 소스로부터 데이터를 **추출(Extract)**, \*\*변환(Transform)\*\*하여 데이터 웨어하우스(DW) 등에 \*\*적재(Load)\*\*하는 기술입니다. 
  * **스쿱 (Sqoop)**: 관계형 데이터베이스(RDBMS)와 하둡 파일 시스템(HDFS) 간에 데이터를 수집하고 전송하는 기술입니다. 
  * **플럼 (Flume)**: 대량의 로그 데이터를 효율적으로 수집, 집계, 이동하기 위해 이벤트와 에이전트를 활용하는 기술입니다. 
  * **크롤링 (Crawling)**: 웹 사이트로부터 웹 문서 및 콘텐츠를 수집하는 기술입니다. 
  * **Open API**: 공개된 API를 통해 실시간으로 데이터를 수신하는 기술입니다. 

-----

### **2. 데이터 유형 및 속성 파악** 

#### **(1) 데이터 유형** 

데이터는 구조, 시간, 저장 형태의 관점에 따라 분류됩니다. 
 

| 관점 | 유형 | 설명 | 예시 |
| :--- | :--- | :--- | :--- |
| **구조 관점**  | **정형 데이터**  | 고정된 필드에 저장된 일관성 있는 데이터  | 관계형 데이터베이스(RDB), 스프레드시트  |
| | **반정형 데이터**  | 스키마 구조를 가지나 일관성은 없는 데이터  | XML, HTML, JSON, 로그 데이터  |
| | **비정형 데이터**  | 정해진 구조가 없는 데이터  | 텍스트, 이미지, 오디오, 비디오, SNS 데이터  |
| **시간 관점**  | **실시간 데이터**  | 생성 후 즉시 처리되어야 의미 있는 데이터  | 센서 데이터, 시스템 로그, 알람  |
| | **비실시간 데이터**  | 일정 시간 후 처리되어도 의미 있는 데이터  | 통계, 웹 로그, 구매 정보  |
| **저장 형태 관점**  | **파일 데이터**  | 파일 형식으로 파일 시스템에 저장된 데이터  | 로그 파일, 텍스트, 스프레드시트  |
| | **데이터베이스 데이터**  | DB의 컬럼이나 테이블에 저장된 데이터  | RDBMS, NoSQL 데이터  |
| | **스트림 데이터**  | 네트워크를 통해 실시간으로 전송되는 데이터  | 센서 데이터, HTTP 트랜잭션  |

-----

### **3. 데이터 비식별화** 

  * **개념**: 특정 개인을 식별할 수 없도록 개인정보의 일부 또는 전부를 변환하는 방법입니다. 
  * **처리 기법**: 가명처리, 총계처리, 데이터 값 삭제, 범주화, 데이터 마스킹 등이 있습니다. 

| 처리 기법 | 설명 | 예시 |
| :--- | :--- | :--- |
| **가명처리**  | 식별 가능한 데이터를 다른 값으로 대체하는 기법  | '홍길동' → '임꺽정'  |
| **총계처리**  | 데이터를 통계값으로 적용하여 특정 개인을 판단할 수 없게 하는 기법  | 'A: 170cm, B: 180cm' → '평균 키: 175cm'  |
| **데이터 값 삭제**  | 개인 식별이 가능한 특정 데이터 값을 삭제하는 기법  | '주민등록번호 901212-1234567' → '90년대생, 남자'  |
| **범주화**  | 단일 식별 정보를 대푯값이나 구간 값으로 변환하는 기법  | '41세' → '40대' 또는 '40\~50세'  |
| **데이터 마스킹**  | 개인 식별 정보의 전체 또는 일부를 대체 값(\*, 공백 등)으로 변환하는 기법  | '홍길동' → '홍\*\*'  |

-----

## **II 빅데이터 탐색**

### **Chapter 01: 데이터 전처리** 

#### **1. 데이터 정제 (Data Cleansing)** 

  * **개념**: 결측값을 채우거나 이상값을 제거하는 과정을 통해 데이터의 신뢰도를 높이는 작업입니다. 
  * **절차**: 데이터 오류 원인 분석 → 정제 대상 선정 → 정제 방법 결정 순으로 진행됩니다. 
